{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN21JvNK4uMMt4KIXtsEroz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilalazhar72/Arxiv-Research-Pooler/blob/main/arxiv_is_my_new_friend_%2C_friendship_with_feedparser_ended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh8i_-oTR8cJ",
        "outputId": "f0fd16ee-e1e9-4dd6-e9a9-8b9f140dff47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (1.4.8)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->arxiv) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTES\n",
        "- so search operators are working which is yay\n",
        "- sometimes cells run but not show any output which is not so POG so definately will have to work on getting this bitch to behave the way i want CHAD <--\n",
        "- i guess check ``stack overflow `` for what the fuck is wrong with this is this a real issue and broken for other people as well or not\n",
        "- run it locally and by that i  mean on github code spaces and then try to figure out the issues as well\n"
      ],
      "metadata": {
        "id": "EgwLacjJeNFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some sick reason the OR was working but when i use and operator with it the code stops working this is really playing with my patience now please god send help"
      ],
      "metadata": {
        "id": "KjWRZIkvfWTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SEMI GOOD**\n",
        "- AND is working fine now\n",
        "- ig try to make another cell and then try to run the ccell to see why th other cell won t run wwell here\n"
      ],
      "metadata": {
        "id": "ZwJinenxfmT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import time\n",
        "import textwrap\n",
        "\n",
        "\n",
        "'''\n",
        "search = arxiv.Search(\n",
        "  query = \"quantum\",\n",
        "  max_results = 10,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "'''\n",
        "# Define the search parameters\n",
        "search = arxiv.Search(\n",
        "    query = 'ti:\"shot\" AND ti:\"train\" cat:cs.AI',\n",
        "    #query='cat:cs.AI',  # Search in the Computer Science category\n",
        "    max_results=10,  # Fetch the 10 most recent articles\n",
        "    sort_by=arxiv.SortCriterion.SubmittedDate,  # Sort by submission date\n",
        "    sort_order=arxiv.SortOrder.Descending  # In descending order, so the most recent articles come first\n",
        ")\n",
        "\n",
        "# Loop through the results and print the title, comment, and PDF URL of each article\n",
        "for result in search.results():\n",
        "    print('Title: ', result.title)\n",
        "    print('Summary: ')\n",
        "    print( textwrap.fill(result.summary, width=220))\n",
        "    print('PDF URL: ', result.pdf_url)\n",
        "\n",
        "    print('Publishing date ', result.published)\n",
        "    print('DOI ', result.doi)\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Sleep for 3 seconds to avoid overloading the server\n",
        "    time.sleep(3)\n"
      ],
      "metadata": {
        "id": "2ZvepE_8-w6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7f3719-03bc-479a-a5be-6954929718a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title:  Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models\n",
            "Summary: \n",
            "Object tracking is central to robot perception and scene understanding. Tracking-by-detection has long been a dominant paradigm for object tracking of specific object categories. Recently, large-scale pre-trained models\n",
            "have shown promising advances in detecting and segmenting objects and parts in 2D static images in the wild. This begs the question: can we re-purpose these large-scale pre-trained static image models for open-vocabulary\n",
            "video tracking? In this paper, we re-purpose an open-vocabulary detector, segmenter, and dense optical flow estimator, into a model that tracks and segments objects of any category in 2D videos. Our method predicts\n",
            "object and part tracks with associated language descriptions in monocular videos, rebuilding the pipeline of Tractor with modern large pre-trained models for static image detection and segmentation: we detect open-\n",
            "vocabulary object instances and propagate their boxes from frame to frame using a flow-based motion model, refine the propagated boxes with the box regression module of the visual detector, and prompt an open-world\n",
            "segmenter with the refined box to segment the objects. We decide the termination of an object track based on the objectness score of the propagated boxes, as well as forward-backward optical flow consistency. We re-\n",
            "identify objects across occlusions using deep feature matching. We show that our model achieves strong performance on multiple established video object segmentation and tracking benchmarks, and can produce reasonable\n",
            "tracks in manipulation data. In particular, our model outperforms previous state-of-the-art in UVO and BURST, benchmarks for open-world object tracking and segmentation, despite never being explicitly trained for\n",
            "tracking. We hope that our approach can serve as a simple and extensible framework for future research.\n",
            "PDF URL:  http://arxiv.org/pdf/2310.06992v1\n",
            "Publishing date  2023-10-10 20:25:30+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation\n",
            "Summary: \n",
            "While pre-training on object detection tasks, such as Common Objects in Contexts (COCO) [1], could significantly boost the performance of cell segmentation, it still consumes on massive fine-annotated cell images [2]\n",
            "with bounding boxes, masks, and cell types for every cell in every image, to fine-tune the pre-trained model. To lower the cost of annotation, this work considers the problem of pre-training DNN models for few-shot cell\n",
            "segmentation, where massive unlabeled cell images are available but only a small proportion is annotated. Hereby, we propose Cross-domain Unsupervised Pre-training, namely CUPre, transferring the capability of object\n",
            "detection and instance segmentation for common visual objects (learned from COCO) to the visual domain of cells using unlabeled images. Given a standard COCO pre-trained network with backbone, neck, and head modules,\n",
            "CUPre adopts an alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in every iteration of pre-training, AMT2 first trains the backbone with cell images from multiple cell datasets via unsupervised\n",
            "momentum contrastive learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets via instance segmentation. After pre-training, CUPre fine-tunes the whole model on the cell segmentation task using a\n",
            "few annotated images. We carry out extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4] datasets in few-shot instance segmentation settings. The experiment shows that CUPre can outperform existing\n",
            "pre-training methods, achieving the highest average precision (AP) for few-shot cell segmentation and detection.\n",
            "PDF URL:  http://arxiv.org/pdf/2310.03981v1\n",
            "Publishing date  2023-10-06 02:35:31+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks\n",
            "Summary: \n",
            "Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing\n",
            "that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel\n",
            "lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data\n",
            "as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting\n",
            "dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using benchmark datasets and representative attacks validates the efficacy of MDP.\n",
            "PDF URL:  http://arxiv.org/pdf/2309.13256v1\n",
            "Publishing date  2023-09-23 04:41:55+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound Detection\n",
            "Summary: \n",
            "Bioacoustic sound event detection allows for better understanding of animal behavior and for better monitoring biodiversity using audio. Deep learning systems can help achieve this goal, however it is difficult to\n",
            "acquire sufficient annotated data to train these systems from scratch. To address this limitation, the Detection and Classification of Acoustic Scenes and Events (DCASE) community has recasted the problem within the\n",
            "framework of few-shot learning and organize an annual challenge for learning to detect animal sounds from only five annotated examples. In this work, we regularize supervised contrastive pre-training to learn features\n",
            "that can transfer well on new target tasks with animal sounds unseen during training, achieving a high F-score of 61.52%(0.48) when no feature adaptation is applied, and an F-score of 68.19%(0.75) when we further adapt\n",
            "the learned features for each new target task. This work aims to lower the entry bar to few-shot bioacoustic sound event detection by proposing a simple and yet effective framework for this task, by also providing open-\n",
            "source code.\n",
            "PDF URL:  http://arxiv.org/pdf/2309.08971v1\n",
            "Publishing date  2023-09-16 12:11:11+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation\n",
            "Summary: \n",
            "In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties\n",
            "necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD\n",
            "approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using\n",
            "few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate\n",
            "that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training. We\n",
            "validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm. We also validate that the haptic representation encoder, pre-trained in simulation, captures\n",
            "the properties of real objects, explaining its contribution to improving the generalisation of the downstream task.\n",
            "PDF URL:  http://arxiv.org/pdf/2309.04640v1\n",
            "Publishing date  2023-09-08 23:42:59+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  Pre-trained Neural Recommenders: A Transferable Zero-Shot Framework for Recommendation Systems\n",
            "Summary: \n",
            "Modern neural collaborative filtering techniques are critical to the success of e-commerce, social media, and content-sharing platforms. However, despite technical advances -- for every new application domain, we need to\n",
            "train an NCF model from scratch. In contrast, pre-trained vision and language models are routinely applied to diverse applications directly (zero-shot) or with limited fine-tuning. Inspired by the impact of pre-trained\n",
            "models, we explore the possibility of pre-trained recommender models that support building recommender systems in new domains, with minimal or no retraining, without the use of any auxiliary user or item information.\n",
            "Zero-shot recommendation without auxiliary information is challenging because we cannot form associations between users and items across datasets when there are no overlapping users or items. Our fundamental insight is\n",
            "that the statistical characteristics of the user-item interaction matrix are universally available across different domains and datasets. Thus, we use the statistical characteristics of the user-item interaction matrix\n",
            "to identify dataset-independent representations for users and items. We show how to learn universal (i.e., supporting zero-shot adaptation without user or item auxiliary information) representations for nodes and edges\n",
            "from the bipartite user-item interaction graph. We learn representations by exploiting the statistical properties of the interaction data, including user and item marginals, and the size and density distributions of\n",
            "their clusters.\n",
            "PDF URL:  http://arxiv.org/pdf/2309.01188v2\n",
            "Publishing date  2023-09-03 14:18:31+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging\n",
            "Summary: \n",
            "We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual\n",
            "descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed\n",
            "by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and\n",
            "visual data.\n",
            "PDF URL:  http://arxiv.org/pdf/2309.01026v2\n",
            "Publishing date  2023-09-02 21:29:53+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation\n",
            "Summary: \n",
            "Accurate automatic segmentation of medical images typically requires large datasets with high-quality annotations, making it less applicable in clinical settings due to limited training data. One-shot segmentation based\n",
            "on learned transformations (OSSLT) has shown promise when labeled data is extremely limited, typically including unsupervised deformable registration, data augmentation with learned registration, and segmentation learned\n",
            "from augmented data. However, current one-shot segmentation methods are challenged by limited data diversity during augmentation, and potential label errors caused by imperfect registration. To address these issues, we\n",
            "propose a novel one-shot medical image segmentation method with adversarial training and label error rectification (AdLER), with the aim of improving the diversity of generated data and correcting label errors to enhance\n",
            "segmentation performance. Specifically, we implement a novel dual consistency constraint to ensure anatomy-aligned registration that lessens registration errors. Furthermore, we develop an adversarial training strategy\n",
            "to augment the atlas image, which ensures both generation diversity and segmentation robustness. We also propose to rectify potential label errors in the augmented atlas images by estimating segmentation uncertainty,\n",
            "which can compensate for the imperfect nature of deformable registration and improve segmentation authenticity. Experiments on the CANDI and ABIDE datasets demonstrate that the proposed AdLER outperforms previous state-\n",
            "of-the-art methods by 0.7% (CANDI), 3.6% (ABIDE \"seen\"), and 4.9% (ABIDE \"unseen\") in segmentation based on Dice scores, respectively. The source code will be available at https://github.com/hsiangyuzhao/AdLER.\n",
            "PDF URL:  http://arxiv.org/pdf/2309.00971v1\n",
            "Publishing date  2023-09-02 16:06:50+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  A Multi-Task Semantic Decomposition Framework with Task-specific Pre-training for Few-Shot NER\n",
            "Summary: \n",
            "The objective of few-shot named entity recognition is to identify named entities with limited labeled instances. Previous works have primarily focused on optimizing the traditional token-wise classification framework,\n",
            "while neglecting the exploration of information based on NER data characteristics. To address this issue, we propose a Multi-Task Semantic Decomposition Framework via Joint Task-specific Pre-training (MSDP) for few-shot\n",
            "NER. Drawing inspiration from demonstration-based and contrastive learning, we introduce two novel pre-training tasks: Demonstration-based Masked Language Modeling (MLM) and Class Contrastive Discrimination. These tasks\n",
            "effectively incorporate entity boundary information and enhance entity representation in Pre-trained Language Models (PLMs). In the downstream main task, we introduce a multi-task joint optimization framework with the\n",
            "semantic decomposing method, which facilitates the model to integrate two different semantic information for entity classification. Experimental results of two few-shot NER benchmarks demonstrate that MSDP consistently\n",
            "outperforms strong baselines by a large margin. Extensive analyses validate the effectiveness and generalization of MSDP.\n",
            "PDF URL:  http://arxiv.org/pdf/2308.14533v1\n",
            "Publishing date  2023-08-28 12:46:21+00:00\n",
            "DOI  None\n",
            "\n",
            "\n",
            "Title:  Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks\n",
            "Summary: \n",
            "To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to few-shot learning. Current 3D few-shot semantic segmentation methods first pre-train the models on `seen' classes, and then\n",
            "evaluate their generalization performance on `unseen' classes. However, the prior pre-training stage not only introduces excessive time overhead, but also incurs a significant domain gap on `unseen' classes. To tackle\n",
            "these issues, we propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and a further training-based variant, TFS3D-T. Without any learnable parameters, TFS3D extracts dense representations by\n",
            "trigonometric positional encodings, and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount\n",
            "of time. Building upon TFS3D, TFS3D-T only requires to train a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate\n",
            "TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency.\n",
            "PDF URL:  http://arxiv.org/pdf/2308.12961v1\n",
            "Publishing date  2023-08-24 17:58:03+00:00\n",
            "DOI  None\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}